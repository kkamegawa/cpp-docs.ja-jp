---
title: 'D: スケジュール句'
ms.date: 01/22/2019
ms.assetid: bf3d8f51-ea05-4803-bf55-657c12e91efe
ms.openlocfilehash: 89e011784c5cccedc4a75f38d553458ea2e5d7e0
ms.sourcegitcommit: 382e247c0f1b4cb7c2dab837b8b6fdff24bff47a
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 01/28/2019
ms.locfileid: "55087289"
---
# <a name="d-the-schedule-clause"></a>D: スケジュール句

並列領域は、少なくとも 1 つのバリアを末尾が、内に別の障壁があります。 各バリアで、チームの他のメンバーは、最後のスレッドが到着するを待つ必要があります。 この待機時間を最小限に抑えるには、すべてのスレッドがほぼ同時にバリアに到達できるように、共有の作業を分散する必要があります。 作業が含まれている場合、共有の一部を`for`構築では、`schedule`句は、この目的に使用できます。

スケジュールの選択、同じオブジェクトを繰り返し参照がある場合に、`for`コンストラクト プレゼンスとキャッシュとメモリ アクセス回数が一定かどうかのサイズなど、メモリ システムの特徴によって主に決定されますか変動的な。 このような考慮事項がなります。 いくつかのスレッドは比較的少ない作業で、ループの一部を割り当てられている場合でも一貫して同じ一連のループ、配列の要素のセットを参照する各スレッドがあることをお勧めです。 使用してこのセットアップを行うことができます、 `static` for ループをすべて同じ境界を持つスケジュールします。 次の例では、0 として提供される 2 つ目のループ内の下限にもかかわらず`k`スケジュールが重要でない場合より自然になります。

```cpp
#pragma omp parallel
{
#pragma omp for schedule(static)
  for(i=0; i<n; i++)
    a[i] = work1(i);
#pragma omp for schedule(static)
  for(i=0; i<n; i++)
    if(i>=k) a[i] += work2(i);
}
```

その他の例では、メモリと想定へのアクセスが主要な考慮事項はありません。 特に明記しない限り、すべてのスレッドが比較可能なコンピューティング リソースを受信します。 スケジュールの選択のこれらのケースで、`for`コンス トラクターは、最も近い先行間で実行するのには、すべての共有作業によって異なりますバリアと暗黙的なバリア、または今後のバリアがある場合に最も近い、 `nowait` 。句。 スケジュールの種類ごとに、簡単な例は、そのスケジュールの種類が最適な選択をする可能性がありますする方法を示します。 簡単な説明では、それぞれの例に従います。

`static`スケジュールが最も簡単な場合は、1 つを含む、並行領域の適切なも`for`同じ量の作業が必要なを構築します。

```cpp
#pragma omp parallel for schedule(static)
for(i=0; i<n; i++) {
  invariant_amount_of_work(i);
}
```

`static`スケジュールの特徴は、プロパティの各スレッドは、約、他のスレッドとのイテレーションの同じ番号を取得して、各スレッドは個別に判断するのに割り当てられるイテレーション。 したがって同期は必要ありません、作業を分散して、各イテレーションが、同じ量の作業が必要なことを前提と、すべてのスレッドは同じ時刻について完了にする必要があります。

チームの*p*のスレッドを使用*ceiling (n/p)* 、整数である*q*、これを満たす*n = p\*q - r* で*0 < = r < p*します。 実装の 1 つ、`static`割り当てることがこの例のスケジュールを設定*q*最初の反復*p-1* 、スレッドと*q r*イテレーションの最後のスレッドにします。  もう 1 つの許容可能な実装を割り当てることが*q*最初のイテレーション*p r*スレッド、および*q-1* 、残りのイテレーション*r*スレッド。 この例では、プログラムが特定の実装の詳細に頼る理由を示します。

`dynamic`スケジュールがの場合に適した、`for`作業量がさまざまな、または予測不能で必要とするイテレーションを作成します。

```cpp
#pragma omp parallel for schedule(dynamic)
  for(i=0; i<n; i++) {
    unpredictable_amount_of_work(i);
}
```

`dynamic`スケジュールが待機しないバリアでよりも長くはその最後の反復処理を実行する別のスレッドのプロパティによって特徴付けられます。 この要件は、イテレーション割り当てる必要があります、一度に 1 つとして、各割り当てのための同期で利用可能になったスレッドを意味します。 最小チャンク サイズを指定することで、同期のオーバーヘッドを削減できます*k*スレッドが割り当てられているように、1 より大きい*k*までよりも少ない、一度に*k*ままにします。 これにより、(1) その最後のチャンクを実行する別のスレッドがより長くバリアでスレッドが待機しない*k*イテレーション。

`dynamic`スケジュールできる、コンピューティング リソースをさまざまなスレッドが表示される場合に役立ちます。 各イテレーションの作業の量は変化とほぼ同じ結果があります。 同様に、動的なスケジュールにも役立ちます、スレッドが到着する場合、`for`がいくつかでこのような場合のさまざまな時間は、構築、`guided`スケジュールが望ましいことがあります。

`guided`スケジュールは、さまざまなタイミングで受信したスレッドをケースに適した、`for`の各反復処理を必要とする同じ量の作業について構築します。 場合に、このような状況が発生することができます、たとえば、`for`コンストラクトの 1 つまたは複数のセクションが付いてまたは`for`構造は`nowait`句。

```cpp
#pragma omp parallel
{
  #pragma omp sections nowait
  {
    // ...
  }
  #pragma omp for schedule(guided)
  for(i=0; i<n; i++) {
    invariant_amount_of_work(i);
  }
}
```

ような`dynamic`、`guided`最後の反復処理を実行する別のスレッドの所要時間よりも長いまたは最終的なバリアでスレッドが待機しないことを保証をスケジュール*k*イテレーション場合のチャンク サイズ*k*を指定します。 このようなスケジュールでは、間、`guided`スケジュールのプロパティで特徴は、最小限の同期が必要であります。 チャンク サイズの*k*、一般的な実装が割り当てられます*q = ceiling (n/p)* 最初の使用可能なスレッドのイテレーションの設定*n* 、大きい方の*n-q*と*p\*k*、割り当てられているすべてのイテレーションになるまで繰り返します。

最適なスケジュールの選択がこれらの例については、それが明確でない場合、`runtime`スケジュールは、さまざまなスケジュールとチャンクのサイズを変更して、プログラムを再コンパイルしなくても試すに便利です。 最適なスケジュールがプログラムを適用する入力データに対して (予測可能な方法はいくつか) に依存する場合に便利ですできます。

さまざまなスケジュール間のトレードオフの例を確認するには、8 つのスレッド間で 1000 回の繰り返しの共有を検討してください。 各イテレーションで作業量がインバリアントであると仮定し、時間の単位として使用します。

すべてのスレッドが、同時に開始する場合、`static`同期なしの 125 単位で実行する構成要素により、スケジュールします。 1 つのスレッドが到着遅延に 100 単位であるとします。 バリアで 100 単位の残りの 7 つのスレッドを待機し、225 に構造体全体の実行に時間が増加します。

ため、両方の`dynamic`と`guided`スケジュールことをバリアで 1 つ以上の単位のスレッドが待機しないこと、遅延のスレッドが可能性がありますから遅延によって増加 138 単位にしか増加コンス トラクターの実行時間が発生し、確認します。同期します。 同期の数が 1000 で重要になる場合このような遅延はごくわずかであり、`dynamic`が唯一の 41 の`guided`、1 つの既定のチャンク サイズを想定します。 チャンク サイズが 25、`dynamic`と`guided`150 の単位と必要な同期を 40 と 20、のみを今すぐ数から遅延それぞれ完了両方。
